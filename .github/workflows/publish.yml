# This workflow will:
# - Create a new Github release
# - Build wheels for supported architectures
# - Deploy the wheels to the Github release
# - Release the static code to PyPi
# For more information see: https://help.github.com/en/actions/language-and-framework-guides/using-python-with-github-actions#publishing-to-package-registries

name: Build wheels and deploy

on:
  create:
    tags:
      - v*

jobs:
  build_wheels:
    name: Build Wheel
    runs-on: ${{ matrix.os }}

    strategy:
      fail-fast: false
      matrix:
          # Using ubuntu-22.04 instead of 24.04 for more compatibility (glibc). Ideally we'd use the
          # manylinux docker image, but I haven't figured out how to install CUDA on manylinux.
          os: [ubuntu-22.04]
          python-version: ['3.9', '3.10', '3.11', '3.12', '3.13']
          torch-version: ['2.4.0', '2.5.1', '2.6.0', '2.7.1', '2.8.0']
          cuda-version: ['12.9.1']
          # We need separate wheels that either uses C++11 ABI (-D_GLIBCXX_USE_CXX11_ABI) or not.
          # Pytorch wheels currently don't use it, but nvcr images have Pytorch compiled with C++11 ABI.
          # Without this we get import error (undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs)
          # when building without C++11 ABI and using it on nvcr images.
          cxx11_abi: ['FALSE', 'TRUE']
          exclude:
            # see https://github.com/pytorch/pytorch/blob/main/RELEASE.md#release-compatibility-matrix
            # Pytorch < 2.5 does not support Python 3.13
            - torch-version: '2.4.0'
              python-version: '3.13'

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Set CUDA and PyTorch versions
        run: |
          # export variables into the hostedrunner environment
          export MATRIX_CUDA_VERSION=$(echo ${{ matrix.cuda-version }} | awk -F \. {'print $1 $2'})
          export MATRIX_TORCH_VERSION=$(echo ${{ matrix.torch-version }} | awk -F \. {'print $1 "." $2'})
          export WHEEL_CUDA_VERSION=$(echo ${{ matrix.cuda-version }} | awk -F \. {'print $1'})
          export MATRIX_PYTHON_VERSION=$(echo ${{ matrix.python-version }} | awk -F \. {'print $1 $2'})
          export TORCH_CUDA_VERSION=$(python -c "from os import environ as env; \
              minv = {'2.4': 118, '2.5': 118, '2.6': 118, '2.7': 118, '2.8': 126}['${MATRIX_TORCH_VERSION}']; \
              maxv = {'2.4': 124, '2.5': 124, '2.6': 126, '2.7': 128, '2.8': 129}['${MATRIX_TORCH_VERSION}']; \
              print(minv if int(${MATRIX_CUDA_VERSION}) < 120 else maxv)" \
            )
          export MAX_JOBS=$([ "$MATRIX_CUDA_VERSION" == "129" ] && echo 1 || echo 2)
          export FLASH_ATTN_LOCAL_VERSION="cu${WHEEL_CUDA_VERSION}torch$(echo ${{ matrix.torch-version }} | awk -F \. {'print $1 $2'})cxx11abi${{ matrix.cxx11_abi }}"
          # Write variables to the GitHub environment
          echo "MATRIX_CUDA_VERSION=${MATRIX_CUDA_VERSION}" >> $GITHUB_ENV
          echo "MATRIX_TORCH_VERSION=${MATRIX_TORCH_VERSION}" >> $GITHUB_ENV
          echo "WHEEL_CUDA_VERSION=${WHEEL_CUDA_VERSION}" >> $GITHUB_ENV
          echo "MATRIX_PYTHON_VERSION=${MATRIX_PYTHON_VERSION}" >> $GITHUB_ENV
          echo "TORCH_CUDA_VERSION=${TORCH_CUDA_VERSION}" >> $GITHUB_ENV
          echo "MAX_JOBS=$MAX_JOBS" >> $GITHUB_ENV
          echo "FLASH_ATTN_LOCAL_VERSION=${FLASH_ATTN_LOCAL_VERSION}" >> $GITHUB_ENV

      - name: Free up disk space
        if: ${{ runner.os == 'Linux' }}
        # https://github.com/easimon/maximize-build-space/blob/master/action.yml
        # https://github.com/easimon/maximize-build-space/tree/test-report
        run: |
          sudo rm -rf /usr/share/dotnet
          sudo rm -rf /opt/ghc
          sudo rm -rf /opt/hostedtoolcache/CodeQL

      - name: Set up swap space
        if: runner.os == 'Linux'
        uses: pierotofy/set-swap-space@v1.0
        with:
          swap-size-gb: 10

      - name: Build wheel
        uses: pypa/cibuildwheel@v3.1.1
        env:
          CIBW_DEBUG_TRACEBACK: 1
          CIBW_BUILD_VERBOSITY: 3
          CIBW_MANYLINUX_X86_64_IMAGE: "manylinux_2_28"
          CIBW_BUILD: "cp${{ env.MATRIX_PYTHON_VERSION }}-manylinux_x86_64"
          CIBW_SKIP: "cp${{ env.MATRIX_PYTHON_VERSION }}-win* cp${{ env.MATRIX_PYTHON_VERSION }}-musl* cp${{ env.MATRIX_PYTHON_VERSION }}-macosx*"
          CIBW_ENVIRONMENT_PASS_LINUX: "MATRIX_CUDA_VERSION MATRIX_TORCH_VERSION WHEEL_CUDA_VERSION MATRIX_PYTHON_VERSION TORCH_CUDA_VERSION FLASH_ATTN_LOCAL_VERSION MAX_JOBS"
          CIBW_ENVIRONMENT: "FLASH_ATTENTION_FORCE_BUILD=TRUE FLASH_ATTENTION_FORCE_CXX11_ABI=${{ matrix.cxx11_abi }} NVCC_THREADS=2"
          CIBW_BUILD_FRONTEND: "pip"
          CIBW_BEFORE_ALL: |
            set -x
            # Check environment variables
            echo "========Checking environment variables========"
            echo "MATRIX_CUDA_VERSION=${MATRIX_CUDA_VERSION}"
            echo "MATRIX_TORCH_VERSION=${MATRIX_TORCH_VERSION}"
            echo "WHEEL_CUDA_VERSION=${WHEEL_CUDA_VERSION}"
            echo "MATRIX_PYTHON_VERSION=${MATRIX_PYTHON_VERSION}"
            echo "TORCH_CUDA_VERSION=${TORCH_CUDA_VERSION}"
            echo "FLASH_ATTN_LOCAL_VERSION=${FLASH_ATTN_LOCAL_VERSION}"
            echo "MAX_JOBS=${MAX_JOBS}"
            echo "FLASH_ATTENTION_FORCE_BUILD=${FLASH_ATTENTION_FORCE_BUILD}"
            echo "FLASH_ATTENTION_FORCE_CXX11_ABI=${FLASH_ATTENTION_FORCE_CXX11_ABI}"
            echo "========Checking environment variables========"
            # Install CUDA toolkit
            dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/rhel8/x86_64/cuda-rhel8.repo
            dnf clean all
            RETRIES=5
            COUNT=0
            # install full cuda-toolkit, maybe cuda-nvcc enough
            until dnf -y install cuda-toolkit; do
              if [[ $COUNT -ge $RETRIES ]]; then
                echo "Failed after $RETRIES attempts, aborting."
                exit 1
              fi
              COUNT=$((COUNT+1))
              echo "Retrying CUDA installation($COUNT/$RETRIES)..."
              sleep 10
            done
            ls -al /usr/local
            export PATH=$PATH:/usr/local/cuda/bin
            nvcc --version
          CIBW_BEFORE_BUILD: |
            # Install PyTorch ${{ matrix.torch-version }}+cu${{ matrix.cuda-version }}
            pip install --upgrade pip

            # With python 3.13 and torch 2.5.1, unless we update typing-extensions, we get error
            # AttributeError: attribute '__default__' of 'typing.ParamSpec' objects is not writable
            pip install typing-extensions==4.12.2
            # We want to figure out the CUDA version to download pytorch
            # e.g. we can have system CUDA version being 11.7 but if torch==1.12 then we need to download the wheel from cu116
            # see https://github.com/pytorch/pytorch/blob/main/RELEASE.md#release-compatibility-matrix
            # This code is ugly, maybe there's a better way to do this.
            
            if [[ ${{ matrix.torch-version }} == *"dev"* ]]; then
              # pip install --no-cache-dir --pre torch==${{ matrix.torch-version }} --index-url https://download.pytorch.org/whl/nightly/cu${TORCH_CUDA_VERSION}
              # Can't use --no-deps because we need cudnn etc.
              # Hard-coding this version of pytorch-triton for torch 2.6.0.dev20241001
              pip install jinja2
              pip install https://download.pytorch.org/whl/nightly/pytorch_triton-3.1.0%2Bcf34004b8a-cp${MATRIX_PYTHON_VERSION}-cp${MATRIX_PYTHON_VERSION}-linux_x86_64.whl
              pip install --no-cache-dir --pre https://download.pytorch.org/whl/nightly/cu${TORCH_CUDA_VERSION}/torch-${{ matrix.torch-version }}%2Bcu${TORCH_CUDA_VERSION}-cp${MATRIX_PYTHON_VERSION}-cp${MATRIX_PYTHON_VERSION}-linux_x86_64.whl
            else
              pip install --no-cache-dir torch==${{ matrix.torch-version }} --index-url https://download.pytorch.org/whl/cu${TORCH_CUDA_VERSION}
            fi
            # We want setuptools >= 49.6.0 otherwise we can't compile the extension if system CUDA version is 11.7 and pytorch cuda version is 11.6
            # https://github.com/pytorch/pytorch/blob/664058fa83f1d8eede5d66418abff6e20bd76ca8/torch/utils/cpp_extension.py#L810
            # However this still fails so I'm using a newer version of setuptools
            pip install 'numpy<2.0'
            pip install ninja packaging wheel setuptools
            export PATH=/usr/local/cuda/bin:/usr/local/cuda/lib64:/usr/local/nvidia/bin:/usr/local/nvidia/lib64:$PATH
            export LD_LIBRARY_PATH=/usr/local/nvidia/lib64:/usr/local/cuda/lib64:$LD_LIBRARY_PATH
            nvcc --version
            python --version
            python -c "import torch; print('PyTorch:', torch.__version__)"
            python -c "import torch; print('CUDA:', torch.version.cuda)"
            python -c "from torch.utils import cpp_extension; print (cpp_extension.CUDA_HOME)"
          CIBW_REPAIR_WHEEL_COMMAND_LINUX: |
            auditwheel repair --exclude 'libcuda.so*' --exclude 'libtorch.so*' --exclude 'libc10.so*' --exclude 'libtorch_python.so*' --exclude 'libc10_cuda.so*' --exclude 'libtorch_cuda.so*' --exclude 'libtorch_cpu.so*' --exclude 'libcudart.so.*' -w {dest_dir} {wheel}
        with:
          output-dir: dist

      - name: List built wheels
        if: ${{ always() }}
        run: |
          ls -lh dist/

      - name: Release
        id: upload_release_asset
        uses: softprops/action-gh-release@v2
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          files: dist/*.whl
      
      # TODO: upload wheels to pypi
      # - name: Deploy package
      #   env:
      #     TWINE_USERNAME: "__token__"
      #     TWINE_PASSWORD: ${{ secrets.PYPI_API_TOKEN }}
      #   run: |
      #     python -m twine upload dist/*.whl

  publish_package:
    name: Publish package
    needs: [build_wheels]

    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          pip install ninja packaging wheel twine
          # Install latest setuptools with support for pypi metadata 2.2 (improved compat w/ uv)
          pip install setuptools==75.8.0
          # We don't want to download anything CUDA-related here
          pip install torch --index-url https://download.pytorch.org/whl/cpu

      - name: Build core package
        env:
          FLASH_ATTENTION_SKIP_CUDA_BUILD: "TRUE"
        run: |
          python setup.py sdist --dist-dir=dist

      - name: Deploy
        env:
          TWINE_USERNAME: "__token__"
          TWINE_PASSWORD: ${{ secrets.PYPI_API_TOKEN }}
        run: |
          python -m twine upload dist/*
