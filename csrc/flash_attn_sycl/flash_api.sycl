#include <c10/xpu/XPUStream.h>
#include <torch/python.h>

std::vector<at::Tensor> mha_fwd(at::Tensor& q,                             // batch_size x seqlen_q x num_heads x round_multiple(head_size, 8)
                                const at::Tensor& k,                       // batch_size x seqlen_k x num_heads_k x round_multiple(head_size, 8)
                                const at::Tensor& v,                       // batch_size x seqlen_k x num_heads_k x round_multiple(head_size, 8)
                                std::optional<at::Tensor>& out_,           // batch_size x seqlen_q x num_heads x round_multiple(head_size, 8)
                                std::optional<at::Tensor>& alibi_slopes_,  // num_heads or batch_size x num_heads
                                const float p_dropout,
                                const float softmax_scale,
                                bool is_causal,
                                int window_size_left,
                                int window_size_right,
                                const float softcap,
                                const bool return_softmax,
                                std::optional<at::Generator> gen_);

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
  m.doc() = "FlashAttention";
  m.def("fwd", &mha_fwd, "Forward pass");
  // m.def("varlen_fwd", &mha_varlen_fwd, "Forward pass (variable length)");
  // m.def("bwd", &mha_bwd, "Backward pass");
  // m.def("varlen_bwd", &mha_varlen_bwd, "Backward pass (variable length)");
  // m.def("fwd_kvcache", &mha_fwd_kvcache, "Forward pass, with KV - cache");
}